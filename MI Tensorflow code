import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import os
import pandas as pd


def model_(evidence,labels):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(64,input_dim=len(evidence[0]),activation='relu' ))
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dense(1,name='savings_at_50'))
    model.compile(optimizer='adam', loss='mean_squared_error')
    
#saving trained model 
    checkpoint_path="./whole{}".format(len(evidence[0]))
    checkpoint_dir=os.path.dirname(checkpoint_path)
    print(checkpoint_dir)
    cp_callback=tf.keras.callbacks.ModelCheckpoint(checkpoint_path,save_model=True,verbose=1)
    model.fit(evidence, labels, epochs=100, callbacks=cp_callback)
#take evidence[0] as example of test value can change 0 to other numbers to mimick other test values
    print("input_test_value, {}".format(evidence[0]))
    print("corresponding predicted savings at 50")
    print(model.predict(np.array(evidence[0]).reshape(1,len(evidence[0]))))
    return (checkpoint_path,model.predict(np.array(evidence[0]).reshape(1,len(evidence[0]))))

# Read training data in from file 
def data_(x):
    with open(r"/Users/glendakoh88gmail.com/Downloads/merge-csv.com__63c2666f327fd.csv") as f:
        reader = csv.reader(f)
        next(reader)
        next(reader)
        next(reader)
        next(reader)
        
        data = []
        exists = []
        for row in reader:
            if row[0] not in exists:
                exists.append(row[0])
                data.append({
                "evidence": [int(row[key]) for key in range(1,x+1)],
                "label": int(row[-1])
            })
        evidence = [row["evidence"] for row in data]
        labels = [row["label"] for row in data]
        evidence=np.array(evidence).reshape(len(evidence),len(evidence[0]))
        labels=np.array(labels).reshape(len(evidence),1)
        
        return model_(evidence,labels)
    
#mimicking an input testing data from users
def datainput_(x):
    with open(r"/Users/glendakoh88gmail.com/Downloads/MOCK_DATA-2.csv") as f:
        reader = csv.reader(f)
        next(reader)
        next(reader)
        next(reader)
        next(reader)
        
        data = []
        exists = []
        for row in reader:
            if row[0] not in exists:
                exists.append(row[0])
                data.append({
                "evidence": [int(row[key]) for key in range(1,x+1)],
                "label": int(row[-1])
            })
        evidence = [row["evidence"] for row in data]
        labels = [row["label"] for row in data]
        evidence=np.array(evidence).reshape(len(evidence),len(evidence[0]))
        labels=np.array(labels).reshape(len(evidence),1)
        
        return evidence
        
def start_(saved_models):
    x=input("the years of data u have from 20 inclusive, best 10 and above!!!:")
    if int(x) not in saved_models:
        n=data_(int(x))
        saved_models[int(x)]=n[0]
        print(saved_models)
        predicted_=n[1]
    else:
        new_model = tf.keras.models.load_model(saved_models[int(x)])
#take evidence[0] as example of test value can change 0 to other numbers to mimick other test values
        print("input_test_value, {}".format(datainput_(int(x))[0]))
        print("corresponding predicted savings at 50")
        predicted_=new_model.predict(np.array(datainput_(int(x))[0]).reshape(1,len(datainput_(int(x))[0])))
        print(predicted_)
        
    insurance_amount=int(input("how much insurance do you have:"))
    age_current=int(input("what is your age:"))
    for i in range(50-age_current):
        insurance_amount=insurance_amount*1.03 
    print("your networth at 50 is {}" .format(insurance_amount+predicted_))
    
    return start_(saved_models)

#creating a dictionary for saved models such that data input of same range of years (e.g savings from 20-25) 
#can be fitted into trained AI models immediately thus only one training for each data form needed  
saved_models={}
start_(saved_models)
